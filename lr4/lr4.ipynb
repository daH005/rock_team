{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bf28ab91-42e7-40fc-88b6-3a14a9cc99bb",
   "metadata": {},
   "source": [
    "Формулировка лаб4: \n",
    "Для датасета с отзывами с маркетплейса на русском языке (https://github.com/sismetanin/rureviews) построить модель для предсказания тональности текста.\n",
    "1. Использовать как минимум 3 модели машинного обучения, решающие задачу классификации (количество классов определить по анализу датасета)\n",
    "2. Предсказать тональность при помощи NLP-моделей (deeppavlov, natasha и т.д.) \n",
    "3. Определить метрики качества моделей и сравнить полученные результаты, в ячейке markdown представить выводы."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "81738114-c2ff-467f-abd9-5ee7c29aed29",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/dan005/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from pathlib import Path\n",
    "\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk.download('wordnet')\n",
    "import pymorphy2\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from deeppavlov import build_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88e8fad5-3ce6-48e8-8364-806ee8e2938a",
   "metadata": {},
   "source": [
    "Датасет скачан из указанного репозитория, считаем его."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "749fbcab-1ae0-41ed-80d4-1b18bd7f941d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('./women-clothing-accessories.3-class.balanced.csv', sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9123e2c3-9d1c-4cbe-b94d-341f35233d5b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>качество плохое пошив ужасный (горловина напер...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Товар отдали другому человеку, я не получила п...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Ужасная синтетика! Тонкая, ничего общего с пре...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>товар не пришел, продавец продлил защиту без м...</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Кофточка голая синтетика, носить не возможно.</td>\n",
       "      <td>negative</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89995</th>\n",
       "      <td>сделано достаточно хорошо. на ткани сделан рис...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89996</th>\n",
       "      <td>Накидка шикарная. Спасибо большое провдо линяе...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89997</th>\n",
       "      <td>спасибо большое ) продовца рекомендую.. заказа...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89998</th>\n",
       "      <td>Очень довольна заказом! Меньше месяца в РБ.  К...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89999</th>\n",
       "      <td>хорошая куртка. постороннего запаха нет. швы р...</td>\n",
       "      <td>positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>90000 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  review sentiment\n",
       "0      качество плохое пошив ужасный (горловина напер...  negative\n",
       "1      Товар отдали другому человеку, я не получила п...  negative\n",
       "2      Ужасная синтетика! Тонкая, ничего общего с пре...  negative\n",
       "3      товар не пришел, продавец продлил защиту без м...  negative\n",
       "4          Кофточка голая синтетика, носить не возможно.  negative\n",
       "...                                                  ...       ...\n",
       "89995  сделано достаточно хорошо. на ткани сделан рис...  positive\n",
       "89996  Накидка шикарная. Спасибо большое провдо линяе...  positive\n",
       "89997  спасибо большое ) продовца рекомендую.. заказа...  positive\n",
       "89998  Очень довольна заказом! Меньше месяца в РБ.  К...  positive\n",
       "89999  хорошая куртка. постороннего запаха нет. швы р...  positive\n",
       "\n",
       "[90000 rows x 2 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6919c32f-3c1b-4b96-8679-ed031c196155",
   "metadata": {},
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ae0fd2e-8e6b-4632-ba29-555ecdcf0aa6",
   "metadata": {},
   "source": [
    "Как видно, строк с NAN'ами нет, это хорошо."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f11a0485-1911-4657-8d31-388e42cebc64",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "negative    30000\n",
       "neautral    30000\n",
       "positive    30000\n",
       "Name: sentiment, dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['sentiment'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "378155a0-365d-43a9-9823-3e2301935cb0",
   "metadata": {},
   "source": [
    "Видим равномерное распределение классов отзывов на 3:\n",
    "- Отрицательные\n",
    "- Нейтральные\n",
    "- Положительные"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cd445a4-fdca-4312-85cf-749f94d95b80",
   "metadata": {},
   "source": [
    "Сделаем стандартное деление на обучающую и тестовую выборки."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0ddbf910-63eb-4f21-981a-4acc76645be7",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(df['review'], df['sentiment'], test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "771fe6e4-5717-4cdf-a32b-c890171dd1cd",
   "metadata": {},
   "source": [
    "Считаем русские стоп-слова для дальнейшей обработки:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "a7d924ab-24e1-4868-9242-9517c10ded75",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['а',\n",
       " 'абсолютно',\n",
       " 'авторизоваться',\n",
       " 'активный',\n",
       " 'алло',\n",
       " 'алтухов',\n",
       " 'атмосфера',\n",
       " 'ах',\n",
       " 'б',\n",
       " 'беду']"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "STOP_WORDS = Path('./stop-ru.txt').read_text().split('\\n')\n",
    "STOP_WORDS[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d2e16e0-ed7b-4cb5-b8a9-57c9dd2983fa",
   "metadata": {},
   "source": [
    "Выполним токенизацию, лемматизацию и векторизацию."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "9d7955c2-1d6e-4aa6-84be-d3ea79ab81d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmer = pymorphy2.MorphAnalyzer()\n",
    "vector = TfidfVectorizer(max_features=5000, stop_words=STOP_WORDS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "f671a815-67b9-4c05-b711-295438ae4fc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clear_and_lemmatize_series(series: pd.Series) -> pd.Series:\n",
    "    \"\"\"Обрабатывает каждое значение `pd.Series` через функцию `_clear_and_lemmatize_text`.\"\"\"\n",
    "    return series.apply(_clear_and_lemmatize_text)\n",
    "\n",
    "\n",
    "def _clear_and_lemmatize_text(text: str) -> str:\n",
    "    \"\"\"Оставляет в тексте только буквы и пробелы, переводит в нижний регистр, а затем лемматизирует его.\"\"\"\n",
    "    return _clear_text(_lemmatize_text(text))\n",
    "    \n",
    "\n",
    "def _clear_text(text: str) -> str:\n",
    "    \"\"\"Оставляет в тексте только буквы и пробелы.\"\"\"\n",
    "    return re.sub(r'[^\\w\\s]', ' ', text)\n",
    "\n",
    "\n",
    "def _lemmatize_text(text: str) -> str:\n",
    "    \"\"\"Переводит текст в нижний регистр, а затем лемматизирует его.\"\"\"\n",
    "    return ' '.join(lemmer.parse(word)[0].normal_form for word in text.lower().split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "63708702-44ee-4ed0-ae75-d18c274e5d0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dan005/rock_team/venv/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:396: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['первых', 'прежнему'] not in stop_words.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "X_train_handled = vector.fit_transform(clear_and_lemmatize_series(X_train))\n",
    "X_test_handled = vector.transform(clear_and_lemmatize_series(X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "562afb73-2ad8-43e3-b248-fbc92c04306e",
   "metadata": {},
   "source": [
    "Применим три модели классификации:\n",
    "- Логистическая регрессия\n",
    "- Случайный лес\n",
    "- Опорные вектора"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "85819c2d-e9e1-494d-9e72-1ac673646994",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression()\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dan005/rock_team/venv/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:\n",
      "0.6996111111111111\n",
      "Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    neautral       0.59      0.61      0.60      6060\n",
      "    negative       0.70      0.68      0.69      5942\n",
      "    positive       0.81      0.80      0.81      5998\n",
      "\n",
      "    accuracy                           0.70     18000\n",
      "   macro avg       0.70      0.70      0.70     18000\n",
      "weighted avg       0.70      0.70      0.70     18000\n",
      "\n",
      "RandomForestClassifier()\n",
      "Accuracy:\n",
      "0.6712777777777778\n",
      "Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    neautral       0.57      0.57      0.57      6060\n",
      "    negative       0.69      0.66      0.68      5942\n",
      "    positive       0.75      0.78      0.76      5998\n",
      "\n",
      "    accuracy                           0.67     18000\n",
      "   macro avg       0.67      0.67      0.67     18000\n",
      "weighted avg       0.67      0.67      0.67     18000\n",
      "\n",
      "SVC()\n",
      "Accuracy:\n",
      "0.7072222222222222\n",
      "Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    neautral       0.59      0.66      0.62      6060\n",
      "    negative       0.72      0.67      0.70      5942\n",
      "    positive       0.83      0.79      0.81      5998\n",
      "\n",
      "    accuracy                           0.71     18000\n",
      "   macro avg       0.71      0.71      0.71     18000\n",
      "weighted avg       0.71      0.71      0.71     18000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "models = [\n",
    "    LogisticRegression(),\n",
    "    RandomForestClassifier(),\n",
    "    SVC(),\n",
    "]\n",
    "\n",
    "for model in models:\n",
    "    print(model)\n",
    "\n",
    "    model.fit(X_train_handled, y_train)\n",
    "    y_pred = model.predict(X_test_handled)\n",
    "\n",
    "    print('Accuracy:')\n",
    "    print(accuracy_score(y_test, y_pred))\n",
    "\n",
    "    print('Report:')\n",
    "    print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43c1d526-ddd2-4912-ade0-48681f02b8de",
   "metadata": {},
   "source": [
    "Рассмотрим теперь DeepPavlov:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a22cb10e-a029-4787-a7ff-c566f28a65aa",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch<1.14.0,>=1.6.0 in /home/dan005/rock_team/venv/lib/python3.10/site-packages (1.13.1)\n",
      "Requirement already satisfied: typing-extensions in /home/dan005/rock_team/venv/lib/python3.10/site-packages (from torch<1.14.0,>=1.6.0) (4.12.2)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu11==11.7.99 in /home/dan005/rock_team/venv/lib/python3.10/site-packages (from torch<1.14.0,>=1.6.0) (11.7.99)\n",
      "Requirement already satisfied: nvidia-cudnn-cu11==8.5.0.96 in /home/dan005/rock_team/venv/lib/python3.10/site-packages (from torch<1.14.0,>=1.6.0) (8.5.0.96)\n",
      "Requirement already satisfied: nvidia-cublas-cu11==11.10.3.66 in /home/dan005/rock_team/venv/lib/python3.10/site-packages (from torch<1.14.0,>=1.6.0) (11.10.3.66)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu11==11.7.99 in /home/dan005/rock_team/venv/lib/python3.10/site-packages (from torch<1.14.0,>=1.6.0) (11.7.99)\n",
      "Requirement already satisfied: setuptools in /home/dan005/rock_team/venv/lib/python3.10/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch<1.14.0,>=1.6.0) (75.6.0)\n",
      "Requirement already satisfied: wheel in /home/dan005/rock_team/venv/lib/python3.10/site-packages (from nvidia-cublas-cu11==11.10.3.66->torch<1.14.0,>=1.6.0) (0.45.1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ignoring transformers: markers 'python_version < \"3.8\"' don't match your environment\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers==4.30.0 in /home/dan005/rock_team/venv/lib/python3.10/site-packages (4.30.0)\n",
      "Requirement already satisfied: filelock in /home/dan005/rock_team/venv/lib/python3.10/site-packages (from transformers==4.30.0) (3.9.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.14.1 in /home/dan005/rock_team/venv/lib/python3.10/site-packages (from transformers==4.30.0) (0.27.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/dan005/rock_team/venv/lib/python3.10/site-packages (from transformers==4.30.0) (1.23.5)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/dan005/rock_team/venv/lib/python3.10/site-packages (from transformers==4.30.0) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/dan005/rock_team/venv/lib/python3.10/site-packages (from transformers==4.30.0) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/dan005/rock_team/venv/lib/python3.10/site-packages (from transformers==4.30.0) (2024.11.6)\n",
      "Requirement already satisfied: requests in /home/dan005/rock_team/venv/lib/python3.10/site-packages (from transformers==4.30.0) (2.32.3)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /home/dan005/rock_team/venv/lib/python3.10/site-packages (from transformers==4.30.0) (0.13.3)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in /home/dan005/rock_team/venv/lib/python3.10/site-packages (from transformers==4.30.0) (0.5.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in /home/dan005/rock_team/venv/lib/python3.10/site-packages (from transformers==4.30.0) (4.64.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /home/dan005/rock_team/venv/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.14.1->transformers==4.30.0) (2024.12.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/dan005/rock_team/venv/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.14.1->transformers==4.30.0) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/dan005/rock_team/venv/lib/python3.10/site-packages (from requests->transformers==4.30.0) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/dan005/rock_team/venv/lib/python3.10/site-packages (from requests->transformers==4.30.0) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/dan005/rock_team/venv/lib/python3.10/site-packages (from requests->transformers==4.30.0) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/dan005/rock_team/venv/lib/python3.10/site-packages (from requests->transformers==4.30.0) (2024.12.14)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.0.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "2025-03-15 19:51:11.188 INFO in 'deeppavlov.download'['download'] at line 138: Skipped http://files.deeppavlov.ai/v1/classifiers/rusentiment_convers_bert/rusentiment_convers_bert_torch.tar.gz download because of matching hashes\n",
      "/home/dan005/rock_team/venv/lib/python3.10/site-packages/huggingface_hub/file_download.py:795: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Some weights of the model checkpoint at DeepPavlov/rubert-base-cased-conversational were not used when initializing BertForSequenceClassification: ['cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at DeepPavlov/rubert-base-cased-conversational and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "2025-03-15 19:51:15.177 WARNING in 'deeppavlov.core.models.torch_model'['torch_model'] at line 96: Unable to place component TorchTransformersClassifierModel on GPU, since no CUDA GPUs are available. Using CPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:\n",
      "0.5757222222222222\n",
      "Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    neautral       0.44      0.81      0.57      6060\n",
      "    negative       0.70      0.37      0.48      5942\n",
      "    positive       0.90      0.54      0.68      5998\n",
      "\n",
      "    accuracy                           0.58     18000\n",
      "   macro avg       0.68      0.57      0.58     18000\n",
      "weighted avg       0.68      0.58      0.58     18000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dp_model = build_model('rusentiment_convers_bert', download=True, install=True)\n",
    "\n",
    "y_pred = [dp_model([x])[0] for x in X_test]\n",
    "\n",
    "# Преобразуем все нижеперечисленные столбцы в унифицированное значение 'neautral'.\n",
    "cols_to_repl = [\n",
    "    'neutral',\n",
    "    'skip',\n",
    "    'speech',\n",
    "]\n",
    "y_pred = ['neautral' if x in cols_to_repl else x for x in y_pred]\n",
    "\n",
    "print('Accuracy:')\n",
    "print(accuracy_score(y_test, y_pred))\n",
    "\n",
    "print('Report:')\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dbd5fe2-6ee3-48c2-a4bc-db4dbccd2070",
   "metadata": {},
   "source": [
    "У нас вышел следующий топ:\n",
    "- Опорные вектора\n",
    "- Логистическая регрессия\n",
    "- Случайный лес\n",
    "- Модель DeepPavlov \"rusentiment_convers_bert\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "920c4b95-d1c7-4e4f-b6d4-5f7bf830cf40",
   "metadata": {},
   "source": [
    "В работе не использовалось специфичных гиперпараметров для настройки моделей и сравнение происходило на параметрах по умолчанию. При таком подходе выиграла модель опорных векторов, она показала наилучшую точность: 0.7."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
